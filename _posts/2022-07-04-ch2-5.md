---
layout: article
aside:
    toc: true
sidebar:
    nav: docs-en
title: 5. Linear Independence
---

# Linear Independence

2.5장에서는 벡터(elements of the vector spaces)를 가지고 무엇을 할 수 있는지에 대해 자세히 살펴보도록 하겠습니다. 벡터끼리 더하거나 벡터에 스칼라 값을 곱할 수 있는데, closure property로 인해서 결과적으로 같은 vector space의 다른 벡터를 얻을 수 있습니다. 따라서, vector space의 모든 벡터를 더하고 스케일링하여 표현할 수 있는 모든 벡터 집합을 찾는 것이 가능합니다. 이러한 벡터 집합을 *basis*(기저)라고 하고, 이는 2.6.1장에서 살펴봅니다. 그전에 먼저 linear combinations(선형 결합)와 linear independece(선형 독립)에 대한 개념을 살펴보도록 하겠습니다.

> **Definition 2.11** (Linear Combination). Vector space $V$ 와 유한한 수의 벡터 $\boldsymbol{x}_1, \dotsc, \boldsymbol{x}_k \in V$ 가 있을 때, 아래의 형태를 만족하는 모든 $\boldsymbol{v} \in V (\lambda_1, \dotsc, \lambda_k \in \mathbb{R})$ 를 벡터 $\boldsymbol{x}_1, \dotsc, \boldsymbol{x}_k$ 의 *linear combination* 이라고 합니다.
> $$ \boldsymbol{v} = \lambda_1\boldsymbol{x}_1 + \cdots + \lambda_k\boldsymbol{x}_k = \sum_{i=1}^k\lambda_i\boldsymbol{x}_i \in V \tag{2.65} $$

$\boldsymbol{0}$ -벡터는 항상 벡터 $\boldsymbol{x}\_1, \dotsc, \boldsymbol{x}\_{k}$ 의 linear combination(선형 결합)으로 표현할 수 있습니다. ( $\boldsymbol{0} = \sum_{i=1}^K 0\boldsymbol{x}_i$ 이 항상 true).

여기서 우리는 벡터 집합에서 자명하지 않은(non-trivial) 선형 결합을 찾는 것을 원합니다. 즉, 계수 $\lambda_i$ 가 전부 0이 아닌 벡터 $\boldsymbol{x}\_1, \dotsc, \boldsymbol{x}\_k$ 의 선형 결합을 찾는 것이 목적입니다.

> **Definition 2.12** (Linear (In)dependence). $k \in \mathbb{N}$ 이고 $\boldsymbol{x}\_1, \dotsc, \boldsymbol{x}\_k$ 인 Vector space $V$ 을 고려했을 때, non-trivial linear combination, 즉, $\boldsymbol{0} = \sum_{i=1}^K \lambda_i \boldsymbol{x}_i$ 에서 적어도 하나의 $\lambda_i$ 가 0이 아니라면 벡터 $\boldsymbol{x}\_1, \dotsc, \boldsymbol{x}\_k$ 를 *linearly dependent*(선형 종속)이라고 합니다. 만약 오직 trivial solution 만 있다면, 즉, $\lambda_1 = \dotsc = \lambda_k = 0$ 이라면, 벡터 $\boldsymbol{x}\_1, \dotsc, \boldsymbol{x}\_k$ 는 *linearly independent*(선형 독립)이라고 합니다.

선형 독립(linear independence)은 선형대수학에서 매우 중요한 개념 중 하나 입니다. 직관적으로 선형 독립인 벡터들의 집합은 중복성(redundancy)이 없는 벡터들로 구성됩니다. 아래에서 조금 더 자세하게 살펴보고 공식화하도록 하겠습니다.

> **Example 2.13** (linearly Dependent Vectors)
> <br>
> 그림으로 살펴보면 선형 독립의 개념을 조금 더 쉽게 이해할 수 있습니다. 아래 그림은 3개의 vector, Kigali-Kampala(374km) / Kampala-Nairobi(506km) / Kigali-Nairobi(751km)를 보여줍니다. 여기서 '506km Northwest' 벡터와 '374km Southwest' 벡터는 각자 서로 다른 벡터를 표현할 수 없기 때문에 두 벡터는 선형 독립(linearly independent) 입니다. 그러나 세 번째 벡터 '751km West'는 다른 두 벡터의 선형 결합으로 표현할 수 있습니다. 따라서, 벡터 집합(3개의 벡터)은 선형 종속(linearly dependent) 입니다.
> <div align="center"><img src="{{ site.baseurl }}/assets/images/figures/figure2.7.png" height=300px></div>


*Remark*. 다음의 속성들은 벡터들이 선형 독립인지 체크하는데 유용합니다.

- $k$ vectors는 linearly dependent 또는 lineary independent 하며, 다른 옵션은 없습니다.
- 만약 벡터 $\boldsymbol{x}_1, \dotsc, \boldsymbol{x}_k$ 중 적어도 하나가 $\boldsymbol{0}$ 이라면 linearly dependent 합니다.
- 벡터 집합 $\lbrace \boldsymbol{x}_1, \dotsc, \boldsymbol{x}_k : \boldsymbol{x}_i \neq \boldsymbol{0}, i = 1, \dotsc, k \rbrace, k \geq 2$ 에서 어느 한 벡터를 이 벡터를 제외한 나머지 벡터들의 선형 결합으로 표현할 수 있다면, 이 벡터 집합은 linear dependent 합니다. 특히, 한 벡터가 다른 벡터의 배수, 즉, $\boldsymbol{x}_i = \lambda\boldsymbol{x}_j$ 라면, 벡터 집합은 linearly dependent 합니다.
- 가우스 소거법(gaussian elimination)을 사용하여 벡터 집합이 lienarly independent 한 지 확인할 수 있습니다. 만약 모든 열이 pivot columns라면 모든 열 벡터들의 집합은 linearly independent 하며, 만약 적어도 하나의 non-pivot column이 존재한다면 열 벡터들의 집합은 linearly dependent 합니다.

> **Example 2.14**
> <br>
> 다음의 벡터들을 고려해봅시다.
> $$ \boldsymbol{x}_1 = \begin{bmatrix} 1 \\ 2 \\ -3 \\ 4 \end{bmatrix}, \boldsymbol{x}_2 = \begin{bmatrix} 1 \\ 1 \\ 0 \\ 2 \end{bmatrix}, \boldsymbol{x}_3 = \begin{bmatrix} -1 \\ -2 \\ 1 \\ 1 \end{bmatrix} \tag{2.67} $$
> 이 벡터들이 선형 종속인지 확인하기 위해서는 일반적으로 다음의 방정식을 풀면 됩니다.
> $$ \lambda_1\boldsymbol{x}_1 + \lambda_2\boldsymbol{x}_2 + \lambda_3\boldsymbol{x}_3 = \lambda_1 \begin{bmatrix} 1 \\ 2 \\ -3 \\ 4 \end{bmatrix} + \lambda_2 \begin{bmatrix} 1 \\ 1 \\ 0 \\ 2 \end{bmatrix} + \lambda_3 \begin{bmatrix} -1 \\ -2 \\ 1 \\ 1 \end{bmatrix} = \boldsymbol{0} \tag{2.68} $$
> 하지만, 벡터 $\boldsymbol{x}_i$ 를 행렬의 열로 작성하고 pivot columns를 구할 때까지 elementary row operation을 적용하면 아래의 결과를 얻을 수 있고,
> $$ \begin{bmatrix} 1 & 1 & -1 \\ 2 & 1 & -2 \\ -3 & 0 & 1 \\ 4 & 2 & 1 \end{bmatrix} \rightsquigarrow \cdots \rightsquigarrow \begin{bmatrix} 1 & 1 & -1 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \\ 0 & 0 & 0 \end{bmatrix} \tag{2.69} $$
> 위 행렬에서 모든 열에 pivot이 존재하므로 식 (2.68)에는 non-trivial solution이 없다는 것을 알 수 있으며, $\lambda_1 = \lambda_2 = \lambda_3 = 0$ 이라는 것을 알 수 있습니다. 따라서 벡터 $\boldsymbol{x}_1, \boldsymbol{x}_2, \boldsymbol{x}_3$ 은 linearly independent 합니다.

*Remark*. 다음의 벡터 공간 $V$ 을 살펴보도록 하겠습니다.

$$ \begin{matrix} \boldsymbol{x}_1 = \sum_{i=1}^k \lambda_{i1}\boldsymbol{b}_i, \\ \vdots \\ \\ \boldsymbol{x}_m = \sum_{i=1}^k \lambda_{im}\boldsymbol{b}_i \end{matrix} \tag{2.70} $$

$\boldsymbol{B} = \lbrack \boldsymbol{b}_1, \dotsc, \boldsymbol{b}_k \rbrack$ 를 모든 열이 선형 독립인 행렬이라고 정의했을 때, 우리는 다음과 같이 작성할 수 있습니다.

$$ \boldsymbol{x}_j = \boldsymbol{B}\boldsymbol{\lambda}_j, \quad \boldsymbol{\lambda}_j = \begin{bmatrix} \lambda_{1j} \\ \vdots \\ \lambda_{kj} \end{bmatrix}, \quad j = 1, \dotsc, m \tag{2.71} $$

여기서 $\boldsymbol{x}\_1, \dotsc, \boldsymbol{x}\_m$ 이 선형 독립인지 확인하려면, 일반적으로 $\sum\_{j=1}^m \psi\_j \boldsymbol{x}\_j = \boldsymbol{0}$ 을 계산해야 합니다. 그리고 식 (2.71)을 통해서 아래의 식을 얻을 수 있습니다.

$$ \sum_{j=1}^m \psi_j \boldsymbol{x}_j = \sum_{j=1}^m \psi_j\boldsymbol{B\lambda}_j = \boldsymbol{B}\sum_{j=1}^m \psi_j\boldsymbol{\lambda}_j \tag{2.72} $$

따라서, 만약 벡터 $\lbrace \boldsymbol{\lambda}\_1, \dotsc, \boldsymbol{\lambda}\_m \rbrace$ 이 선형 독립이라면, $\lbrace \boldsymbol{x}\_1, \dotsc, \boldsymbol{x}\_m \rbrace$ 도 선형 독립이라는 의미가 됩니다.

*Remark*. 벡터 공간 $V$ 에서, $k$ 개의 벡터 $\boldsymbol{x}_1, \dotsc, \boldsymbol{x}_k$ 에서 m개의 선형 결합이 있을 때, $m > k$ 라면 m개의 선형 결합의 벡터 집합은 linearly dependent 합니다.

> **Example 2.15**
> <br>
> 선형 독립인 벡터 집합 $\boldsymbol{b}_1, \boldsymbol{b}_2, \boldsymbol{b}_3, \boldsymbol{b}_4 \in \mathbb{R}^n$ 과 다음의 식을 살펴보겠습니다.
> $$ \begin{matrix} \boldsymbol{x}_1 & = &\boldsymbol{b}_1 &- &2\boldsymbol{b}_2 &+ &\boldsymbol{b}_3 &- &\boldsymbol{b}_4 \\ \boldsymbol{x}_2 &= &-4\boldsymbol{b}_1 &- &2\boldsymbol{b}_2 & & &+ &4\boldsymbol{b}_4 \\ \boldsymbol{x}_3 &= &2\boldsymbol{b}_1 &+ &3\boldsymbol{b}_2 &- &\boldsymbol{b}_3 &- &3\boldsymbol{b}_4 \\ \boldsymbol{x}_4 &= &17\boldsymbol{b}_1 &- &10\boldsymbol{b}_2 &+ &11\boldsymbol{b}_3 &+ &\boldsymbol{b}_4 \end{matrix} \tag{2.73} $$
> $\boldsymbol{x}_1, \boldsymbol{x}_2, \boldsymbol{x}_3, \boldsymbol{x}_4 \in \mathbb{R}^n$ 이 선형 독립인지 알아보려면, 아래의 열 벡터들이 선형 독립인지 확인해보면 됩니다.
> $$ \left\lbrace \begin{bmatrix} 1 \\ -2 \\ 1 \\ -1 \end{bmatrix}, \begin{bmatrix} -4 \\ 2 \\ 0 \\ 4 \end{bmatrix}, \begin{bmatrix} 2 \\ 3 \\ -1 \\ -3 \end{bmatrix}, \begin{bmatrix} 17 \\ -10 \\ 11 \\ 1 \end{bmatrix} \right\rbrace  \tag{2.74} $$ 
> 따라서, 아래의 연립선형방정식을 reduced row-echelon form으로 변환하면,
> $$ \boldsymbol{A} = \begin{bmatrix} 1 & -4 & 2 & 17 \\ -2 & -2 & 3 & -10 \\ 1 & 0 & -1 & 11 \\ -1 & 4 & -3 & 1 \end{bmatrix} \tag{2.75} $$
> 은 다음과 같이 변환됩니다.
> $$ \boldsymbol{A} = \begin{bmatrix} 1 & 0 & 0 & -7 \\ 0 & 1 & 0 & -15 \\ 0 & 0 & 1 & 18 \\ 0 & 0 & 0 & 0 \end{bmatrix} \tag{2.76} $$
> 마지막 열에는 pivot이 없기 때문에 해당 연립선형방정식은 non-trivially solvable이라는 것을 알 수 있으며, $\boldsymbol{x}_4 = -7\boldsymbol{x}_1 -15\boldsymbol{x}_2 -18\boldsymbol{x}_3$ 입니다. 
> 따라서, $\boldsymbol{x}_1, \boldsymbol{x}_2, \boldsymbol{x}_3, \boldsymbol{x}_4$ 는 선형 종속(linearly dependent)이며, $\boldsymbol{x}_4$ 는 $\boldsymbol{x}_1, \boldsymbol{x}_2, \boldsymbol{x}_3$ 의 선형 결합으로 표현될 수 있습니다.