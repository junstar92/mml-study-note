---
layout: article
aside:
    toc: true
sidebar:
    nav: docs-en
title: 5. Orthonormal Basis
---

# Orthonormal Basis

2.6.1장에서 우리는 basis vectors의 특징들을 살펴봤고, n-차원의 vector space에서 n개의 basis vectors가 필요하다는 것을 확인했습니다. 즉, n개의 vector가 서로 linearly independent 합니다. 3.3장과 3.4장에서는 내적을 사용하여 벡터의 길이와 두 벡터 간의 각도를 계산했습니다. 이번 장에서는 basis vectors가 서로 직교(orthogonal)하는 경우와 각 basis vector의 길이가 1인 경우에 대해 살펴봅니다. 그리고 이렇게 서로 직교하는 basis들을 *orthonormal basis* 라고 부릅니다.

> **Definition 3.9** (Orthonormal Basis). n-차원의 vector space $V$ 와 $V$ 의 basis $\lbrace\boldsymbol{b}_1,\dotsc,\boldsymbol{b}_n\rbrace$ 가 있을 때, 모든 $i, j = 1,\dotsc,n$ 에서 다음(3.33, 3.34)이 성립할 때,
> 
> $$ \begin{align*} &\langle\boldsymbol{b}_i,\boldsymbol{b}_j\rangle = 0 \quad \text{for } i \neq j \tag{3.33} \\ &\langle\boldsymbol{b}_i, \boldsymbol{b}_i\rangle = 1 \tag{3.34} \end{align*} $$
> 
> 이 basis를 ***orthonormal basis***(ONB) 라고 합니다.
> 
> 만약 (3.33)만 만족한다면, 이 basis를 *orthogonal basis* 라고 합니다. (3.34)를 만족한다면, 모든 basis vector의 length/norm은 1이라는 것을 암시합니다.

<br>

2.6.1장에서는 가우스 소거법(Gaussian elimination)을 사용하여 주어진 vector space의 basis vector를 찾을 수 있다고 했습니다.

non-orthogonal이면서 unnormalized한 basis vector 집합 $\lbrace\tilde{\boldsymbol{b}}_1,\dotsc,\tilde{\boldsymbol{b}}_n\rbrace$ 이 주어졌다고 가정해봅시다. 이때, basis vector들을 합쳐서 행렬 $\tilde{\boldsymbol{B}} = \lbrack\tilde{\boldsymbol{b}}_1,\dotsc,\tilde{\boldsymbol{b}}_n\rbrack$ 를 만들고, augmented matrix $\lbrack\tilde{\boldsymbol{B}}\tilde{\boldsymbol{B}}^\top\|\tilde{\boldsymbol{B}}\rbrack$ 에 가우스 소거법을 적용하면 orthonormal basis를 얻을 수 있습니다.

이 방법을 *Gram-Schmidt process* 라고 합니다. 이 방법을 통해 직교하지 않으면서 정규화되지 않은, 즉, 길이가 1이 아닌 basis vector를 orthonormal basis vector로 변환할 수 있습니다.

> **Example 3.8** (Orthonormal Basis)
> <br>
> Euclidean vector space $\mathbb{R}^n$ 의 canonical/standard basis는 orthonormal basis 입니다. 여기서 내적은 dot product 입니다.
> 
> $\mathbb{R}^n$ 에서 다음의 벡터
> 
> $$ \boldsymbol{b}_1 = \frac{1}{\sqrt{2}}\begin{bmatrix}1\\1\end{bmatrix}, \quad \boldsymbol{b}_2 = \frac{1}{\sqrt{2}}\begin{bmatrix}1\\-1\end{bmatrix} \tag{3.35} $$
> 
> 는 orthonormal basis를 형성합니다. ($\boldsymbol{b}_1^\top\boldsymbol{b}_2 = 0$ and $\\|\boldsymbol{b}_1\\| = 1 = \\|\boldsymbol{b}_2\\|$ 이므로)

<br>

10장의 주성분 분석(principal component analysis)과 12장의 서포트 벡터 머신(SVM, support vector machine)에 대해 논의할 때 orthonormal basis의 개념을 활용합니다.