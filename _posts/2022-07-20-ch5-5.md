---
layout: article
aside:
    toc: true
sidebar:
    nav: docs-en
title: 5. Useful Identities for Computing Gradients
---

# Useful Identities for Computing Gradients

이번 장에서는 머신러닝에서 자주 사용되는 유용한 gradient들을 정리합니다. 여기서 $tr(\cdot)$ 은 trace이고, $\det(\cdot)$ 은 determinant 입니다 ([4.1장]({{ site.baseurl }}/2022/07/12/ch4-1.html#trace) 참조). $\boldsymbol{f}(\boldsymbol{X})^{-1}$ 은 $\boldsymbol{f}(\boldsymbol{X})$ 의 inverse이며, 존재한다고 가정합니다.

$$ \begin{align*} &\frac{\partial}{\partial\boldsymbol{X}}\boldsymbol{f}(\boldsymbol{X})^\top = \left(\frac{\partial\boldsymbol{f}(\boldsymbol{X})}{\partial\boldsymbol{X}}\right)^\top \tag{5.99} \\ &\frac{\partial}{\partial\boldsymbol{X}}\text{tr}(\boldsymbol{f}(\boldsymbol{X})) = \text{tr}\left(\frac{\partial\boldsymbol{f}(\boldsymbol{X})}{\partial\boldsymbol{X}}\right) \tag{5.100} \\ &\frac{\partial}{\partial\boldsymbol{X}}\det(\boldsymbol{f}(\boldsymbol{X})) = \det(\boldsymbol{f}(\boldsymbol{X}))\text{tr}\left(\boldsymbol{f}(\boldsymbol{X})^{-1}\frac{\partial\boldsymbol{f}(\boldsymbol{X})}{\partial\boldsymbol{X}}\right) \tag{5.101} \\ &\frac{\partial}{\partial\boldsymbol{X}}\boldsymbol{f}(\boldsymbol{X})^{-1} = -\boldsymbol{f}(\boldsymbol{X})^{-1}\frac{\partial\boldsymbol{f}(\boldsymbol{X})}{\partial\boldsymbol{X}}\boldsymbol{f}(\boldsymbol{X})^{-1} \tag{5.102} \\ &\frac{\partial\boldsymbol{a}^\top\boldsymbol{X}^{-1}\boldsymbol{b}}{\partial\boldsymbol{X}} = -(\boldsymbol{X}^{-1})^\top\boldsymbol{ab}^\top(\boldsymbol{X}^{-1})^\top \tag{5.103} \\ &\frac{\partial\boldsymbol{x}^\top\boldsymbol{a}}{\partial\boldsymbol{x}} = \boldsymbol{a}^\top \tag{5.104} \\ &\frac{\partial\boldsymbol{a}^\top\boldsymbol{x}}{\partial\boldsymbol{x}} = \boldsymbol{a}^\top \tag{5.105} \\ &\frac{\partial\boldsymbol{a}^\top\boldsymbol{Xb}}{\partial\boldsymbol{X}} = \boldsymbol{ab}^\top \tag{5.106} \\ &\frac{\partial\boldsymbol{x}^\top\boldsymbol{Bx}}{\partial\boldsymbol{x}} = \boldsymbol{x}^\top(\boldsymbol{B}+\boldsymbol{B}^\top) \tag{5.107} \\ &\frac{\partial}{\partial\boldsymbol{s}}(\boldsymbol{x}-\boldsymbol{As})^\top\boldsymbol{W}(\boldsymbol{x}-\boldsymbol{As}) = -2(\boldsymbol{x}-\boldsymbol{As})^\top\boldsymbol{WA} \quad\text{ for symmetric } \boldsymbol{W} \tag{5.108} \end{align*} $$

*Remark*. 이 책에서는 오직 행렬의 traces와 traspose만 다룹니다. 그러나 derivatives는 더 높은 차원의 텐서일 수 있으며 이러한 경우에서는 일반적인 trace와 transpose가 정의되지 않습니다.

$D\times D\times E\times F$ 텐서에서 trace는 $E\times F$ 차원의 행렬이 됩니다. 이는 tensor contraction의 특별한 케이스입니다. 비슷하게 텐서를 "transpose" 한다는 것은 처음 나오는 두 차원을 스왑한다는 의미입니다.