---
layout: article
aside:
    toc: true
sidebar:
    nav: docs-en
title: 4. Gradients of Matrices
---

# Gradients of Matrices

Vectors(or matrices)에 대한 gradient matrix가 필요한 상황이 있을 수 있으며, 이는 결과적으로 multidimensional tensor(다차원 텐서)를 생성합니다. 이러한 텐서는 partial derivatives 결과들이 모인 multidimensional array로 생각할 수 있습니다. 예를 들어, $p\times q$ 행렬 $\boldsymbol{B}$ 에 대한 $m \times n$ 행렬 $\boldsymbol{A}$ 의 gradient를 계산한다면, 그 결과는 $(m\times n)\times(p \times q)$ 인 Jacobian 입니다. 즉, 4차원 텐서 $\boldsymbol{J}$ 가 되며, 이 텐서의 요소들은 다음과 같이 정의됩니다.

$$ J_{ijkl} = \frac{\partial A_{ij}}{\partial B_{kl}} $$

행렬은 linear mapping으로 표현될 수 있기 때문에, $m\times n$ 행렬들의 space $\mathbb{R}^{m\times n}$ 과 $mn$ 벡터들의 space $\mathbb{R}^{mn}$ 사이에 vector-space isomorphism(linear, invertible mapping)이 있다는 사실을 활용할 수 있습니다.

따라서, 행렬을 각각 length가 $mn$ 인 벡터와 $pq$ 인 벡터로 re-shape할 수 있습니다. $mn$ vectors를 사용하여 계산한 gradient는 $mn\times pq$ 크기의 Jacobian이 됩니다. 아래 그림은 두 가지 접근 방법을 사용하는 방법에 대해 시각적으로 보여줍니다.

<div align="center"><img src="{{ site.baseurl }}/assets/images/figures/figure5.7a.png" width=500px></div>

각 편도함수 $\frac{\partial\boldsymbol{A}}{\partial x_1}, \frac{\partial\boldsymbol{A}}{\partial x_2}, \frac{\partial\boldsymbol{A}}{\partial x_3}$ 은 $4\times2$ 행렬이고, 이를 결합하면 $4\times2\times3$ 텐서가 됩니다.

<br>
<div align="center"><img src="{{ site.baseurl }}/assets/images/figures/figure5.7b.png" width=500px></div>

행렬 $\boldsymbol{A}$ 를 $\tilde{\boldsymbol{A}} \in \mathbb{R}^{8}$ 로 re-shape(flatten)한 뒤, gradient $\frac{\mathrm{d}\tilde{\boldsymbol{A}}}{\mathrm{d}\boldsymbol{x}}\in\mathbb{R}^{8\times3}$ 을 계산합니다. 그리고 위 그림에서 보여주는 것처럼 다시 re-shape하면 $4\times2\times3$ 텐서의 gradient를 얻을 수 있습니다.

<br>

> **Example 5.12** (Gradient of Vectors with Respect to Matrices)
> 
> $$ \boldsymbol{f} = \boldsymbol{Ax}, \quad \boldsymbol{f}\in\mathbb{R}^{M},\quad\boldsymbol{A}\in\mathbb{R}^{M\times N}, \quad\boldsymbol{x}\in\mathbb{R}^N \tag{5.85} $$
> 
> 위와 같이 주어졌을 때, gradient $\mathrm{d}\boldsymbol{f}/\mathrm{d}\boldsymbol{A}$ 를 계산해보도록 하겠습니다.
> 
> 먼저, 다음과 같이 gradient의 차원을 먼저 찾습니다.
> 
> $$ \frac{\mathrm{d}\boldsymbol{f}}{\mathrm{d}\boldsymbol{A}}\in\mathbb{R}^{M\times(M\times N)} \tag{5.86} $$
> 
> Gradient는 partial derivatives의 collection이므로, 다음과 같습니다.
> 
> $$ \frac{\mathrm{d}\boldsymbol{f}}{\mathrm{d}\boldsymbol{A}} = \begin{bmatrix}\frac{\partial f_1}{\partial\boldsymbol{A}}\\\vdots\\\frac{\partial f_M}{\partial\boldsymbol{A}}\end{bmatrix}, \quad \frac{\partial f_i}{\partial\boldsymbol{A}}\in\mathbb{R}^{1\times(M\times N)} \tag{5.87} $$
> 
> Partial derivatives를 계산할 때, 아래와 같이 행렬-벡터 곱으로 작성하는 것이 편리합니다.
> 
> $$ f_i = \sum_{j=1}^N A_{ij}x_j, \quad i=1,\dotsc,M \tag{5.88} $$
> 
> 그리고, partial derivatives는 다음과 같이 주어집니다.
> 
> $$ \frac{\partial f_i}{\partial A_{iq}} = x_q  \tag{5.89} $$
> 
> 그러면 $\boldsymbol{A}$ 의 row에 대해 $f_i$ 의 partial derivatives는 아래와 같이 계산할 수 있습니다.
> 
> $$ \frac{\partial f_i}{\partial A_{i,:}} = \boldsymbol{x}^\top \in \mathbb{R}^{1\times1\times N} \tag{5.90} $$
> 
> $$ \frac{\partial f_i}{\partial A_{k\neq i, :}} = \boldsymbol{0}^\top \in \mathbb{R}^{1\times1\times N} \tag{5.91} $$
> 
> 여기서 차원이 올바른지 주의해야 합니다. $f_i$ 는 $\mathbb{R}$ 로 매핑하고 $\boldsymbol{A}$ 의 각 행은 $1\times N$ 이므로, $\boldsymbol{A}$ 의 행에 대한 $f_i$ 의 partial derivatives로 $1\times1\times N$ 크기의 텐서를 얻습니다.
> 
> 따라서, (5.91)의 결과를 쌓아서 다음의 결과를 통해 최종 gradient를 얻을 수 있습니다.
> 
> $$ \frac{\partial f_i}{\partial \boldsymbol{A}} = \begin{bmatrix}\boldsymbol{0}^\top\\\vdots\\\boldsymbol{0}^\top\\\boldsymbol{x}^\top\\\boldsymbol{0}^\top\\\vdots\\\boldsymbol{0}^\top\end{bmatrix}\in\mathbb{R}^{1\times(M\times N)} \tag{5.92} $$

<br>

> **Example 5.13** (Gradient of Matrices with Respect to Matrices)
> 
> 이번에는 벡터가 아닌 행렬의 gradient에 대해 살펴봅니다.
> 
> 행렬 $\boldsymbol{R}\in\mathbb{R}^{M\times N}$ 과 아래와 같이 주어지는 함수 $\boldsymbol{f}:\mathbb{R}^{M\times N}\rightarrow\mathbb{R}^{N\times N}$ 이 있을 때,
> 
> $$ \boldsymbol{f}(\boldsymbol{R}) = \boldsymbol{R}^\top\boldsymbol{R} =: \boldsymbol{K}\in\mathbb{R}^{N\times N} \tag{5.93} $$
> 
> gradient $\mathrm{d}\boldsymbol{K}/\mathrm{d}\boldsymbol{R}$ 을 찾아보도록 하겠습니다.
> 
> Gradient를 구하기 위해서 먼저 아래와 같이 gradient의 차원을 결정합니다.
> 
> $$ \frac{\mathrm{d}\boldsymbol{K}}{\mathrm{d}\boldsymbol{R}}\in\mathbb{R}^{(N\times N)\times(M\times N)} \tag{5.94} $$
> 
> 즉, gradient는 텐서(tensor) 입니다.
> 
> 또한, $\boldsymbol{K}=\boldsymbol{f}(\boldsymbol{R})$ 의 $(p, q)$ th entry $K_{pq}$ 에서의 gradient의 차원은 다음과 같습니다 ($p, q = 1,\dotsc,N$ ).
> 
> $$ \frac{\mathrm{d}K_{pq}}{\mathrm{d}\boldsymbol{R}}\in\mathbb{R}^{1\times M\times N} \tag{5.95} $$
> 
> $\boldsymbol{R}$ 의 $i$ 번째 column을 $\boldsymbol{r}_i$ 라고 표기한다면, $\boldsymbol{K}$ 의 모든 요소들은 아래와 같이 $\boldsymbol{R}$ 의 두 column의 dot product로 주어집니다.
> 
> $$ K_{pq} = \boldsymbol{r}_p^\top\boldsymbol{r}_q = \sum_{m=1}^M R_{mp}R_{mq} \tag{5.96} $$
> 
> 이제 편도함수 $\frac{\partial K_{pq}}{\partial R_{ij}}$ 를 계산하면, 다음의 결과를 얻을 수 있습니다.
> 
> $$ \frac{\partial K_{pq}}{\partial R_{ij}} = \sum_{m=1}^M \frac{\partial}{\partial R_{ij}}R_{mp}R_{mq} = \partial_{pqij} \tag{5.97} $$
> 
> $$ \partial_{pqij} = \left\lbrace \begin{align*}R_{iq}\quad&\text{if }j = p, p\neq q \\R_{ip} \quad &\text{if }j = q, p\neq q\\2R_{iq}\quad&\text{if }j = p, p=q\\0\quad&\text{otherwise}\end{align*} \right. \tag{5.98} $$

