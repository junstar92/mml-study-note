---
layout: article
aside:
    toc: true
sidebar:
    nav: docs-en
title: 0. Intro
---

# Intro

Machine Learning 에서의 많은 알고리즘들은 목적 함수(objective function)를 최적화합니다. 이 목적 함수는 모델이 데이터를 얼마나 잘 설명하는지를 컨트롤하는 모델의 파라미터와 관련되어 있습니다. 여기서, 좋은 파라미터를 찾는 것을 최적화 문제로 표현할 수 있습니다.

이와 관련한 예시들이 몇 가지 있는데, 다음과 같습니다. 

(1) Linear Regression (9장 참조): 선형 회귀라고 부르며, 9장에서는 curve-fitting problem을 살펴보고 likelihood를 최대화하도록 linear weight 파라미터를 최적화합니다.

(2) Neural network auto-encoders for dimensionality reduction and data compression: 여기서 파라미터는 각 layer의 weight와 bias 이며, chain rule을 반복적으로 적용하여 reconstruction error를 최소화하도록 최적화합니다.

(3) Gaussian mixture models for modeling data distributions (11장 참조): 여기서는 모델의 likelihood를 최대화하기 위해 각 mixture component의 location and shape 파라미터를 최적화합니다.

<div align="center"><img src="{{ site.baseurl }}/assets/images/figures/figure5.1.png" height=250px></div>

위 그림은 위에서 예시로 든 문제들을 보여주며, 일반적으로 기울기(gradient)를 이용하는 최적화 알고리즘을 사용하여 해결합니다 (7.1장 참조).

아래 그림은 5장에서 배우는 개념들이 어떻게 관련되며, 다른 챕터들과는 어떠한 연관성이 있는지 보여줍니다.

<div align="center"><img src="{{ site.baseurl }}/assets/images/figures/figure5.2.png" height=500px></div>

<br>

이 장의 핵심은 **the concept of a function** 입니다. 함수 $f$ 는 두 quantities를 서로 연관시키는 quantity 입니다. 이 책에서는 이러한 quantities를 일반적으로 입력인 $\boldsymbol{x}\in\mathbb{R}^{D}$ 와 targets(함수값) $f(\boldsymbol{x})$ 이며, 따로 언급하지 않는다면 실수(real number)라고 가정합니다. 여기서 $\mathbb{R}^{D}$ 는 $f$ 의 *domain* 이고, 함수값(function values) $f(\boldsymbol{x})$ 는 $f$ 의 *image/codomain* 입니다.

2.7.3장에서는 linear functions이라는 맥락에서 조금 더 자세하게 다루는데, 함수를 종종 다음과 같이 표현합니다.

$$ \begin{align*} f : \mathbb{R}^{D} &\rightarrow \mathbb{R} \tag{5.1a} \\ \boldsymbol{x} &\rightarrow f(\boldsymbol{x}) \tag{5.1b} \end{align*} $$

이는 함수 $f$ 를 $\mathbb{R}^{D}$ 에서 $\mathbb{R}$ 로의 mapping으로 취급한다는 것을 의미합니다.

> **Example 5.1**
> <br>
> [3.2장]({{ site.baseurl }}/2022/07/07/ch3-2.html)에서 살펴봤던 inner product의 special case인 dot product를 떠올려봅시다. Dot product를 위에서 언급한 함수의 notion으로 표현하면 $f(\boldsymbol{x}) = \boldsymbol{x}^\top\boldsymbol{x}, \quad \boldsymbol{x}\in\mathbb{R}^2$ 로 표현할 수 있습니다. 즉, 다음과 같습니다.
> 
> $$ \begin{align*} f : \mathbb{R}^{2} &\rightarrow \mathbb{R} \tag{5.2a} \\ \boldsymbol{x} &\rightarrow x_1^2 + x_2^2 \tag{5.2b} \end{align*} $$

<br>

5장에서는 함수의 gradients(기울기)를 계산하는 방법에 대해서도 다룹니다. 기울기가 가장 가파른 방향을 가리키기 때문에, 기울기는 머신러닝 모델을 학습하는데 필수적입니다. 따라서, 벡터의 미적분학(vector calculus)는 머신러닝에서 기본적인 수학적 도구 중 하나입니다. 이 책에서는 함수들이 미분 가능하다고 가정합니다. 7장에서는 제약(contraints)이 있는 함수로 확장하여 살펴봅니다.